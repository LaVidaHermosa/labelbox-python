{
  "nbformat": 4,
  "nbformat_minor": 1,
  "metadata": {},
  "cells": [
    {
      "metadata": {},
      "source": [
        "# Getting started \n",
        "## Importing a labeled dataset\n",
        "---"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "!pip install \"labelbox[data]\""
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "import labelbox as lb\n",
        "import labelbox.types as lb_types\n",
        "import requests\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "from uuid import uuid4\n",
        "import datetime\n",
        "import random"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Generic data download function\n",
        "def download_files(filemap):\n",
        "    path, uri = filemap    \n",
        "    ## Download data\n",
        "    if not os.path.exists(path):\n",
        "        r = requests.get(uri, stream=True)\n",
        "        if r.status_code == 200:\n",
        "            with open(path, 'wb') as f:\n",
        "                for chunk in r:\n",
        "                    f.write(chunk)\n",
        "    return path"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Setup Labelbox client"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "## Generate API key: https://app.labelbox.com/account/api-keys\n",
        "LB_API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VySWQiOiJjbG9vcmRpaGUwMDkyMDcza2Nvcm5jajdnIiwib3JnYW5pemF0aW9uSWQiOiJjbG9vcmRpZ3cwMDkxMDcza2M2cG9oeWFiIiwiYXBpS2V5SWQiOiJjbHE1OWd6M3MwMDRxMDcweDRwb3BmajV4Iiwic2VjcmV0IjoiOWE5ZWVmNDczNDI2ZDI2ZjUwOTU5ZDY4ZmZiNGJmMWMiLCJpYXQiOjE3MDI1NjIwMjYsImV4cCI6MjMzMzcxNDAyNn0.BsdKnIr8Np4eYxJ_6VILmuY-D6n2gUdvGKGvMHq9Eh4\"\n",
        "client = lb.Client(LB_API_KEY)\n",
        "\n",
        "DATA_ROWS = \"https://storage.googleapis.com/labelbox-datasets/VHR_geospatial/geospatial_datarows.json\"\n",
        "ANNOTATIONS = \"https://storage.googleapis.com/labelbox-datasets/VHR_geospatial/geospatial_annotations.json\""
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Download a public dataset"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "download_files((\"data_rows.json\", DATA_ROWS))\n",
        "download_files((\"annotations.json\", ANNOTATIONS))"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'annotations.json'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "with open('data_rows.json', 'r') as fp:\n",
        "    data_rows = json.load(fp)\n",
        "\n",
        "with open('annotations.json', 'r') as fp:\n",
        "    annotations = json.load(fp)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Create a dataset"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "dataset = client.create_dataset(name=\"Geospatial vessel detection\")"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Import Data Rows with Metadata"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "# Here is an example of adding two metadata fields to your Data Rows: a \"captureDateTime\" field with datetime value, and a \"tag\" field with string value\n",
        "metadata_ontology = client.get_data_row_metadata_ontology()\n",
        "datetime_schema_id = metadata_ontology.reserved_by_name[\"captureDateTime\"].uid\n",
        "tag_schema_id = metadata_ontology.reserved_by_name[\"tag\"].uid\n",
        "tag_items = [\"WorldView-1\", \"WorldView-2\", \"WorldView-3\", \"WorldView-4\"]\n",
        "\n",
        "for datarow in data_rows:\n",
        "    dt = datetime.datetime.utcnow() + datetime.timedelta(days=random.random()*30) # this is random datetime value\n",
        "    tag_item = random.choice(tag_items) # this is a random tag value\n",
        "\n",
        "    # Option 1: Specify metadata with a list of DataRowMetadataField. This is the recommended option since it comes with validation for metadata fields.\n",
        "    metadata_fields = [\n",
        "                       lb.DataRowMetadataField(schema_id=datetime_schema_id, value=dt), \n",
        "                       lb.DataRowMetadataField(schema_id=tag_schema_id, value=tag_item)\n",
        "                       ]\n",
        "\n",
        "    # Option 2: Uncomment to try. Alternatively, you can specify the metadata fields with dictionary format without declaring the DataRowMetadataField objects. It is equivalent to Option 1.\n",
        "    # metadata_fields = [\n",
        "    #                    {\"schema_id\": datetime_schema_id, \"value\": dt}, \n",
        "    #                    {\"schema_id\": tag_schema_id, \"value\": tag_item}\n",
        "    #                    ]\n",
        "\n",
        "    datarow[\"metadata_fields\"] = metadata_fields"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "task = dataset.create_data_rows(data_rows)\n",
        "task.wait_till_done()"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "Examine a Data Row"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "datarow = next(dataset.data_rows())\n",
        "print(datarow)"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<DataRow {\n",
            "    \"created_at\": \"2024-01-30 18:54:53.691000+00:00\",\n",
            "    \"external_id\": \"positive_image_set/053.jpg\",\n",
            "    \"global_key\": null,\n",
            "    \"media_attributes\": {\n",
            "        \"assetType\": \"image\",\n",
            "        \"contentLength\": 102486,\n",
            "        \"height\": 578,\n",
            "        \"mimeType\": \"image/jpeg\",\n",
            "        \"subType\": \"jpeg\",\n",
            "        \"superType\": \"image\",\n",
            "        \"width\": 1025\n",
            "    },\n",
            "    \"metadata\": [\n",
            "        \"schema_id='cko8s9r5v0001h2dk9elqdidh' name='tag' value='WorldView-3'\",\n",
            "        \"schema_id='cko8sdzv70006h2dk8jg64zvb' name='captureDateTime' value=datetime.datetime(2024, 2, 14, 21, 14, 18, tzinfo=tzutc())\"\n",
            "    ],\n",
            "    \"metadata_fields\": [\n",
            "        {\n",
            "            \"schemaId\": \"cko8s9r5v0001h2dk9elqdidh\",\n",
            "            \"name\": \"tag\",\n",
            "            \"value\": \"WorldView-3\",\n",
            "            \"kind\": \"CustomMetadataString\"\n",
            "        },\n",
            "        {\n",
            "            \"schemaId\": \"cko8sdzv70006h2dk8jg64zvb\",\n",
            "            \"name\": \"captureDateTime\",\n",
            "            \"value\": \"2024-02-14T21:14:18Z\",\n",
            "            \"kind\": \"CustomMetadataDateTime\"\n",
            "        }\n",
            "    ],\n",
            "    \"row_data\": \"https://storage.googleapis.com/labelbox-datasets/VHR_geospatial/positive_image_set/053.jpg\",\n",
            "    \"uid\": \"cls0px98e011v07257q99q0v2\",\n",
            "    \"updated_at\": \"2024-01-30 18:54:55.936000+00:00\"\n",
            "}>\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Setup a labeling project"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "ontology = lb.OntologyBuilder()\n",
        "\n",
        "for tool in annotations['categories']:\n",
        "  print(tool['name'])\n",
        "  ontology.add_tool(lb.Tool(tool = lb.Tool.Type.BBOX, name = tool['name']))\n",
        "\n",
        "ontology = client.create_ontology(\"Vessel detection ontology\", ontology.asdict())\n",
        "project = client.create_project(name=\"Vessel detection\", media_type=lb.MediaType.Image)\n",
        "project.setup_editor(ontology)\n",
        "ontology_from_project = lb.OntologyBuilder.from_project(project)"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "airplane\n",
            "ship\n",
            "storage_tank\n",
            "baseball_diamond\n",
            "tennis_court\n",
            "basketball_court\n",
            "ground_track_field\n",
            "harbor\n",
            "bridge\n",
            "vehicle\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "### Prepare and queue batch of Data Rows to the project"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "#### Export Dataset"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "client.enable_experimental = True\n",
        "\n",
        "export_task = dataset.export()\n",
        "export_task.wait_till_done()"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "#### Stream Data Rows"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "data_rows_ids = []\n",
        "\n",
        "def json_stream_handler(output: lb.JsonConverterOutput):\n",
        "  data_row = json.loads(output.json_str)\n",
        "  data_rows_ids.append(data_row[\"data_row\"][\"id\"])\n",
        "\n",
        "\n",
        "if export_task.has_errors():\n",
        "  export_task.get_stream(\n",
        "  converter=lb.JsonConverter(),\n",
        "  stream_type=lb.StreamType.ERRORS\n",
        "  ).start(stream_handler=lambda error: print(error))\n",
        "\n",
        "if export_task.has_result():\n",
        "  export_json = export_task.get_stream(\n",
        "    converter=lb.JsonConverter(),\n",
        "    stream_type=lb.StreamType.RESULT\n",
        "  ).start(stream_handler=json_stream_handler)\n"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Randomly select 200 Data Rows\n",
        "sampled_data_rows = random.sample(data_rows_ids, 200)\n",
        "\n",
        "batch = project.create_batch(\n",
        "  f\"Batch-{str(uuid4())}\", # name of the batch\n",
        "  sampled_data_rows, # list of Data Rows\n",
        "  1 # priority between 1-5\n",
        ")"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "ename": "ResourceConflict",
          "evalue": "Batch with name 'Initial batch' already exists in this project(\"Batch with name 'Initial batch' already exists in this project\", None)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceConflict\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[35], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Randomly select 200 Data Rows\u001b[39;00m\n\u001b[1;32m      2\u001b[0m sampled_data_rows \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(data_rows_ids, \u001b[38;5;241m200\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[43mproject\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mInitial batch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# name of the batch\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m  \u001b[49m\u001b[43msampled_data_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# list of Data Rows\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# priority between 1-5\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/repos/labelbox-python/labelbox/schema/project.py:851\u001b[0m, in \u001b[0;36mProject.create_batch\u001b[0;34m(self, name, data_rows, priority, consensus_settings, global_keys)\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_batch_async(name, dr_ids, global_keys, priority,\n\u001b[1;32m    849\u001b[0m                                     consensus_settings)\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 851\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_batch_sync\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdr_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mconsensus_settings\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/repos/labelbox-python/labelbox/schema/project.py:1028\u001b[0m, in \u001b[0;36mProject._create_batch_sync\u001b[0;34m(self, name, dr_ids, global_keys, priority, consensus_settings)\u001b[0m\n\u001b[1;32m   1007\u001b[0m query_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mmutation \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124mPyApi($projectId: ID!, $batchInput: CreateBatchInput!) \u001b[39m\u001b[38;5;124m{\u001b[39m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;124m          project(where: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mid: $projectId}) \u001b[39m\u001b[38;5;124m{\u001b[39m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;124m            \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m(input: $batchInput) \u001b[39m\u001b[38;5;124m{\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;124m        }\u001b[39m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m \u001b[38;5;241m%\u001b[39m (method, method, query\u001b[38;5;241m.\u001b[39mresults_query_part(Entity\u001b[38;5;241m.\u001b[39mBatch))\n\u001b[1;32m   1018\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1019\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprojectId\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muid,\n\u001b[1;32m   1020\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatchInput\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1026\u001b[0m     }\n\u001b[1;32m   1027\u001b[0m }\n\u001b[0;32m-> 1028\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m180.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mexperimental\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproject\u001b[39m\u001b[38;5;124m\"\u001b[39m][method]\n\u001b[1;32m   1032\u001b[0m batch \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m   1033\u001b[0m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
            "File \u001b[0;32m~/repos/labelbox-python/env/lib/python3.10/site-packages/google/api_core/retry.py:372\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    369\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    371\u001b[0m )\n\u001b[0;32m--> 372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/repos/labelbox-python/env/lib/python3.10/site-packages/google/api_core/retry.py:207\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 207\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[1;32m    209\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
            "File \u001b[0;32m~/repos/labelbox-python/labelbox/client.py:283\u001b[0m, in \u001b[0;36mClient.execute\u001b[0;34m(self, query, params, data, files, timeout, experimental, error_log_key)\u001b[0m\n\u001b[1;32m    280\u001b[0m resource_conflict_error \u001b[38;5;241m=\u001b[39m check_errors([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRESOURCE_CONFLICT\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    281\u001b[0m                                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextensions\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resource_conflict_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 283\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m labelbox\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mResourceConflict(\n\u001b[1;32m    284\u001b[0m         resource_conflict_error[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    286\u001b[0m malformed_request_error \u001b[38;5;241m=\u001b[39m check_errors([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMALFORMED_REQUEST\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    287\u001b[0m                                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextensions\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m malformed_request_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[0;31mResourceConflict\u001b[0m: Batch with name 'Initial batch' already exists in this project(\"Batch with name 'Initial batch' already exists in this project\", None)"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Process ground truth annotations for import"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "queued_data_rows = project.export_queued_data_rows()\n",
        "ground_truth_list = list()\n",
        "\n",
        "for datarow in queued_data_rows:\n",
        "  annotations_list = []\n",
        "  folder = datarow['externalId'].split(\"/\")[0]\n",
        "  id = datarow['externalId'].split(\"/\")[1]\n",
        "  if folder == \"positive_image_set\":\n",
        "    for image in annotations['images']:\n",
        "      if (image['file_name']==id):\n",
        "        for annotation in annotations['annotations']:\n",
        "          if annotation['image_id'] == image['id']:\n",
        "            bbox = annotation['bbox']\n",
        "            id = annotation['category_id'] - 1\n",
        "            class_name = ontology_from_project.tools[id].name\n",
        "            annotations_list.append(lb_types.ObjectAnnotation(\n",
        "                name = class_name,\n",
        "                value = lb_types.Rectangle(start = lb_types.Point(x = bbox[0], y = bbox[1]), end = lb_types.Point(x = bbox[2]+bbox[0], y = bbox[3]+bbox[1])),\n",
        "            ))\n",
        "  image = lb_types.ImageData(uid = datarow['id'])\n",
        "  ground_truth_list.append(lb_types.Label(data = image, annotations = annotations_list))"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Import ground truth annotation"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "start_time = time.time()\n",
        "## Upload annotations\n",
        "upload_task = lb.LabelImport.create_from_objects(client, project.uid, \"geospatial-import-job-1\", ground_truth_list)\n",
        "print(upload_task)\n",
        "\n",
        "#Wait for upload to finish (Will take up to five minutes)\n",
        "upload_task.wait_until_done()\n",
        "print(upload_task.errors)\n",
        "print(\"--- Finished in %s mins ---\" % ((time.time() - start_time)/60))"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# queued_data_rows = [dr['id'] for dr in list(project.export_queued_data_rows())]\n",
        "# data_rows = [dr.uid for dr in list(dataset.export_data_rows())]\n",
        "# data_rows_not_queued = list(set(data_rows)- set(queued_data_rows))\n",
        "\n",
        "# # Randomly select 200 Data Rows\n",
        "# sampled_data_rows = random.sample(data_rows_not_queued, 200)\n",
        "\n",
        "# batch = project.create_batch(\n",
        "#   \"Second batch\", # name of the batch\n",
        "#   sampled_data_rows, # list of Data Rows\n",
        "#   5 # priority between 1-5\n",
        "# )\n"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    }
  ]
}