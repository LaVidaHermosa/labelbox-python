{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5Zex-ktX5ME"
   },
   "source": [
    "# Getting started \n",
    "## Importing a labeled dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "U6rdGGv8m-lz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: labelbox[data]\n"
     ]
    }
   ],
   "source": [
    "!pip3 install labelbox[data]\n",
    "import labelbox\n",
    "from labelbox.schema.ontology import OntologyBuilder, Tool, Classification,Option\n",
    "from labelbox.schema.annotation_import import MALPredictionImport\n",
    "from labelbox.data.serialization import NDJsonConverter\n",
    "from labelbox.schema.annotation_import import LabelImport\n",
    "from labelbox.schema.queue_mode import QueueMode\n",
    "from labelbox.schema.media_type import MediaType\n",
    "from labelbox import LabelingFrontend\n",
    "from labelbox.data.annotation_types import (\n",
    "    Label,\n",
    "    Point,\n",
    "    ImageData,\n",
    "    Rectangle,\n",
    "    ObjectAnnotation,\n",
    ")\n",
    "from labelbox.schema.data_row_metadata import (\n",
    "    DataRowMetadata,\n",
    "    DataRowMetadataField,\n",
    "    DeleteDataRowMetadata,\n",
    "    DataRowMetadataKind\n",
    ")\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HiKZnB4YPqHj"
   },
   "outputs": [],
   "source": [
    "## Generic data download function\n",
    "def download_files(filemap):\n",
    "    path, uri = filemap    \n",
    "    ## Download data\n",
    "    if not os.path.exists(path):\n",
    "        r = requests.get(uri, stream=True)\n",
    "        if r.status_code == 200:\n",
    "            with open(path, 'wb') as f:\n",
    "                for chunk in r:\n",
    "                    f.write(chunk)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqJt-G_aS-Z-"
   },
   "source": [
    "# Setup Labelbox client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wiwPkXe4nJrS"
   },
   "outputs": [],
   "source": [
    "## Generate API key: https://app.labelbox.com/account/api-keys\n",
    "import os\n",
    "client = labelbox.Client(api_key=os.environ['LABELBOX_TEST_API_KEY_LOCAL'], endpoint=\"http://localhost:8080/graphql\")\n",
    "\n",
    "DATA_ROWS = \"https://storage.googleapis.com/labelbox-datasets/VHR_geospatial/geospatial_datarows.json\"\n",
    "ANNOTATIONS = \"https://storage.googleapis.com/labelbox-datasets/VHR_geospatial/geospatial_annotations.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5kXlyspS5GR"
   },
   "source": [
    "# Download a public dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UEXN2pyt3YZQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'annotations.json'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_files((\"data_rows.json\", DATA_ROWS))\n",
    "download_files((\"annotations.json\", ANNOTATIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OfWoTuKeUX8n"
   },
   "outputs": [],
   "source": [
    "with open('data_rows.json', 'r') as fp:\n",
    "    data_rows = json.load(fp)\n",
    "\n",
    "with open('annotations.json', 'r') as fp:\n",
    "    annotations = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-VfjDaRSYtk"
   },
   "source": [
    "# Create a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ldzBOurIn8Ff"
   },
   "outputs": [],
   "source": [
    "dataset = client.create_dataset(name=\"Geospatial vessel detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zewMUN9WScOv"
   },
   "source": [
    "# Import Data Rows with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "5_CrtIGqUimL"
   },
   "outputs": [],
   "source": [
    "task = dataset.create_data_rows(data_rows)\n",
    "task.wait_till_done()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpF4vH4eUkPt"
   },
   "source": [
    "Examine a Data Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "HGdJZLYoUmKH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DataRow {\n",
      "    \"created_at\": \"2023-02-15 00:36:08+00:00\",\n",
      "    \"external_id\": \"positive_image_set/497.jpg\",\n",
      "    \"global_key\": null,\n",
      "    \"media_attributes\": {},\n",
      "    \"metadata\": [],\n",
      "    \"metadata_fields\": [],\n",
      "    \"row_data\": \"https://storage.googleapis.com/labelbox-datasets/VHR_geospatial/positive_image_set/497.jpg\",\n",
      "    \"uid\": \"cle4xzy7s02ijtjzmcj278blv\",\n",
      "    \"updated_at\": \"2023-02-15 00:36:08+00:00\"\n",
      "}>\n"
     ]
    }
   ],
   "source": [
    "datarow = next(dataset.data_rows())\n",
    "print(datarow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4VjfJKESeoi"
   },
   "source": [
    "# Setup a labeling project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "4PR2sWSL-L7b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default createProject behavior will soon be adjusted to prefer batch projects. Pass in `queue_mode` parameter explicitly to opt-out for the time being.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airplane\n",
      "ship\n",
      "storage_tank\n",
      "baseball_diamond\n",
      "tennis_court\n",
      "basketball_court\n",
      "ground_track_field\n",
      "harbor\n",
      "bridge\n",
      "vehicle\n"
     ]
    }
   ],
   "source": [
    "ontology = OntologyBuilder()\n",
    "\n",
    "for tool in annotations['categories']:\n",
    "  print(tool['name'])\n",
    "  ontology.add_tool(Tool(tool = Tool.Type.BBOX, name = tool['name']))\n",
    "\n",
    "ontology = client.create_ontology(\"Vessel detection ontology\", ontology.asdict())\n",
    "project = client.create_project(name=\"Vessel detection\", media_type=MediaType.Image)\n",
    "project.setup_editor(ontology)\n",
    "ontology_from_project = OntologyBuilder.from_project(project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvq-ndx1HSDN"
   },
   "source": [
    "Prepare and queue batch of Data Rows to the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "YARsP1NZHRWj"
   },
   "outputs": [
    {
     "ename": "ResourceConflict",
     "evalue": "Batch with name 'Initial batch' already exists in this project(\"Batch with name 'Initial batch' already exists in this project\", None)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceConflict\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Randomly select 200 Data Rows\u001b[39;00m\n\u001b[1;32m      4\u001b[0m sampled_data_rows \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39msample(data_rows, \u001b[39m200\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m batch \u001b[39m=\u001b[39m project\u001b[39m.\u001b[39;49mcreate_batch(\n\u001b[1;32m      7\u001b[0m   \u001b[39m\"\u001b[39;49m\u001b[39mInitial batch\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m# name of the batch\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m   sampled_data_rows, \u001b[39m# list of Data Rows\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m   \u001b[39m1\u001b[39;49m \u001b[39m# priority between 1-5\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/site-packages/labelbox/schema/project.py:700\u001b[0m, in \u001b[0;36mProject.create_batch\u001b[0;34m(self, name, data_rows, priority, consensus_settings)\u001b[0m\n\u001b[1;32m    697\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_batch_async(name, dr_ids, priority,\n\u001b[1;32m    698\u001b[0m                                     consensus_settings)\n\u001b[1;32m    699\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 700\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_batch_sync(name, dr_ids, priority,\n\u001b[1;32m    701\u001b[0m                                    consensus_settings)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/site-packages/labelbox/schema/project.py:725\u001b[0m, in \u001b[0;36mProject._create_batch_sync\u001b[0;34m(self, name, dr_ids, priority, consensus_settings)\u001b[0m\n\u001b[1;32m    705\u001b[0m query_str \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\u001b[39mmutation \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39mPyApi($projectId: ID!, $batchInput: CreateBatchInput!) \u001b[39m\u001b[39m{\u001b[39m\n\u001b[1;32m    706\u001b[0m \u001b[39m          project(where: \u001b[39m\u001b[39m{\u001b[39m\u001b[39mid: $projectId}) \u001b[39m\u001b[39m{\u001b[39m\n\u001b[1;32m    707\u001b[0m \u001b[39m            \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m(input: $batchInput) \u001b[39m\u001b[39m{\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[39m        }\u001b[39m\n\u001b[1;32m    715\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39m \u001b[39m%\u001b[39m (method, method, query\u001b[39m.\u001b[39mresults_query_part(Entity\u001b[39m.\u001b[39mBatch))\n\u001b[1;32m    716\u001b[0m params \u001b[39m=\u001b[39m {\n\u001b[1;32m    717\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mprojectId\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muid,\n\u001b[1;32m    718\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mbatchInput\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    723\u001b[0m     }\n\u001b[1;32m    724\u001b[0m }\n\u001b[0;32m--> 725\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mexecute(query_str,\n\u001b[1;32m    726\u001b[0m                           params,\n\u001b[1;32m    727\u001b[0m                           timeout\u001b[39m=\u001b[39;49m\u001b[39m180.0\u001b[39;49m,\n\u001b[1;32m    728\u001b[0m                           experimental\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)[\u001b[39m\"\u001b[39m\u001b[39mproject\u001b[39m\u001b[39m\"\u001b[39m][method]\n\u001b[1;32m    729\u001b[0m batch \u001b[39m=\u001b[39m res[\u001b[39m'\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    730\u001b[0m batch[\u001b[39m'\u001b[39m\u001b[39msize\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(dr_ids)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/site-packages/google/api_core/retry.py:349\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m target \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mpartial(func, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    346\u001b[0m sleep_generator \u001b[39m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    347\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maximum, multiplier\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_multiplier\n\u001b[1;32m    348\u001b[0m )\n\u001b[0;32m--> 349\u001b[0m \u001b[39mreturn\u001b[39;00m retry_target(\n\u001b[1;32m    350\u001b[0m     target,\n\u001b[1;32m    351\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predicate,\n\u001b[1;32m    352\u001b[0m     sleep_generator,\n\u001b[1;32m    353\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_timeout,\n\u001b[1;32m    354\u001b[0m     on_error\u001b[39m=\u001b[39;49mon_error,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/site-packages/google/api_core/retry.py:191\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39mfor\u001b[39;00m sleep \u001b[39min\u001b[39;00m sleep_generator:\n\u001b[1;32m    190\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 191\u001b[0m         \u001b[39mreturn\u001b[39;00m target()\n\u001b[1;32m    193\u001b[0m     \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[39m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/site-packages/labelbox/client.py:259\u001b[0m, in \u001b[0;36mClient.execute\u001b[0;34m(self, query, params, data, files, timeout, experimental)\u001b[0m\n\u001b[1;32m    256\u001b[0m resource_conflict_error \u001b[39m=\u001b[39m check_errors([\u001b[39m\"\u001b[39m\u001b[39mRESOURCE_CONFLICT\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    257\u001b[0m                                        \u001b[39m\"\u001b[39m\u001b[39mextensions\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    258\u001b[0m \u001b[39mif\u001b[39;00m resource_conflict_error \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 259\u001b[0m     \u001b[39mraise\u001b[39;00m labelbox\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mResourceConflict(\n\u001b[1;32m    260\u001b[0m         resource_conflict_error[\u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    262\u001b[0m malformed_request_error \u001b[39m=\u001b[39m check_errors([\u001b[39m\"\u001b[39m\u001b[39mMALFORMED_REQUEST\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    263\u001b[0m                                        \u001b[39m\"\u001b[39m\u001b[39mextensions\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    264\u001b[0m \u001b[39mif\u001b[39;00m malformed_request_error \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mResourceConflict\u001b[0m: Batch with name 'Initial batch' already exists in this project(\"Batch with name 'Initial batch' already exists in this project\", None)"
     ]
    }
   ],
   "source": [
    "data_rows = [dr.uid for dr in list(dataset.export_data_rows())]\n",
    "\n",
    "# Randomly select 200 Data Rows\n",
    "sampled_data_rows = random.sample(data_rows, 200)\n",
    "\n",
    "batch = project.create_batch(\n",
    "  \"Initial batch\", # name of the batch\n",
    "  sampled_data_rows, # list of Data Rows\n",
    "  1 # priority between 1-5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkCFk7qUSu53"
   },
   "source": [
    "# Process ground truth annotations for import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "fmeFTCk7-Vrn"
   },
   "outputs": [],
   "source": [
    "queued_data_rows = project.export_queued_data_rows()\n",
    "ground_truth_list = list()\n",
    "\n",
    "for datarow in queued_data_rows:\n",
    "  annotations_list = []\n",
    "  folder = datarow['externalId'].split(\"/\")[0]\n",
    "  id = datarow['externalId'].split(\"/\")[1]\n",
    "  if folder == \"positive_image_set\":\n",
    "    for image in annotations['images']:\n",
    "      if (image['file_name']==id):\n",
    "        for annotation in annotations['annotations']:\n",
    "          if annotation['image_id'] == image['id']:\n",
    "            bbox = annotation['bbox']\n",
    "            id = annotation['category_id'] - 1\n",
    "            class_name = ontology_from_project.tools[id].name\n",
    "            annotations_list.append(ObjectAnnotation(\n",
    "                name = class_name,\n",
    "                value = Rectangle(start = Point(x = bbox[0], y = bbox[1]), end = Point(x = bbox[2]+bbox[0], y = bbox[3]+bbox[1])),\n",
    "            ))\n",
    "  image = ImageData(uid = datarow['id'])\n",
    "  ground_truth_list.append(Label(data = image, annotations = annotations_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCDxiydHSzeN"
   },
   "source": [
    "# Import ground truth annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "71WXNj6FB60-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LabelImport {\n",
      "    \"error_file_url\": null,\n",
      "    \"input_file_url\": \"https://storage.googleapis.com/lb-dev0-na-us-import/uploaded_predictions/cldos55nl000ckuu377xc8b65/d89112c6-877e-db9d-a85d-030122987dad-cle4y04f102j9tjzm8fo824in__geospatial-import-job-1.ndjson?GoogleAccessId=sa-lb-api-0da28c-7505d6%40lb-dev0-na-us.iam.gserviceaccount.com&Expires=1677026550&Signature=s8nChdOGNCYrY4yUVvabuwYcEXVCQKEyDUyvSnIhS4c8fiv4KGuxU%2FOaSLHNtaKZu10Mjy6PQtv23Tq4rO%2F2l23%2BB%2BP5awhRTDNdI5zKSk1xlb%2BY74pqp0MtBNicGWvlY1LSbeoy6nry20yaDMYOdO777Q71oAKemk1BL%2BA1iWzs%2BZcuObD3FEs1zDQDxk1Y%2BZEQvBdEpNZgjFJ1AmEthRx2BopUo%2Fx6lwonNDWWVu8MvSohwMVsxFEVX2WnDlcoZhL%2FAJROxYq7SBtEy2jAUW%2Bhg5eLDIq1AYNq8HH6ra0mfMTBXEqTcS0SgnNAmRup%2FBhBaysnWq%2B7W4lIoAewRw%3D%3D\",\n",
      "    \"name\": \"geospatial-import-job-1\",\n",
      "    \"progress\": null,\n",
      "    \"state\": \"AnnotationImportState.RUNNING\",\n",
      "    \"status_file_url\": null,\n",
      "    \"uid\": \"a0996f74-9780-0a1c-726d-054a1d6efd75\"\n",
      "}>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x111eb6e60>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/valbro13/.pyenv/versions/3.10.9/lib/python3.10/site-packages/tqdm/std.py\", line 1162, in __del__\n",
      "    self.close()\n",
      "  File \"/Users/valbro13/.pyenv/versions/3.10.9/lib/python3.10/site-packages/tqdm/notebook.py\", line 288, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'disp'\n",
      "Exception ignored in: <function tqdm.__del__ at 0x111eb6e60>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/valbro13/.pyenv/versions/3.10.9/lib/python3.10/site-packages/tqdm/std.py\", line 1162, in __del__\n",
      "    self.close()\n",
      "  File \"/Users/valbro13/.pyenv/versions/3.10.9/lib/python3.10/site-packages/tqdm/notebook.py\", line 288, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'disp'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Import failed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39m#Wait for upload to finish (Will take up to five minutes)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m upload_task\u001b[39m.\u001b[39mwait_until_done()\n\u001b[0;32m---> 10\u001b[0m \u001b[39mprint\u001b[39m(upload_task\u001b[39m.\u001b[39;49merrors)\n\u001b[1;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m--- Finished in \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m mins ---\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m ((time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time)\u001b[39m/\u001b[39m\u001b[39m60\u001b[39m))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/site-packages/labelbox/schema/annotation_import.py:55\u001b[0m, in \u001b[0;36mAnnotationImport.errors\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39mErrors for each individual annotation uploaded. This is a subset of statuses\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[39mReturns:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39m* This information will expire after 24 hours.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwait_until_done()\n\u001b[0;32m---> 55\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fetch_remote_ndjson(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_file_url)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/site-packages/labelbox/schema/annotation_import.py:126\u001b[0m, in \u001b[0;36mAnnotationImport._fetch_remote_ndjson\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[39mFetches the remote ndjson file and caches the results.\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39m    ndjson as a list of dicts.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m AnnotationImportState\u001b[39m.\u001b[39mFAILED:\n\u001b[0;32m--> 126\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImport failed.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    128\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(url)\n\u001b[1;32m    129\u001b[0m response\u001b[39m.\u001b[39mraise_for_status()\n",
      "\u001b[0;31mValueError\u001b[0m: Import failed."
     ]
    }
   ],
   "source": [
    "ground_truth_ndjson = list(NDJsonConverter.serialize(ground_truth_list))\n",
    "print(ground_truth_ndjson)\n",
    "start_time = time.time()\n",
    "## Upload annotations\n",
    "upload_task = LabelImport.create_from_objects(client, project.uid, \"geospatial-import-job-1\", ground_truth_ndjson)\n",
    "print(upload_task)\n",
    "\n",
    "#Wait for upload to finish (Will take up to five minutes)\n",
    "upload_task.wait_until_done()\n",
    "print(upload_task.errors)\n",
    "print(\"--- Finished in %s mins ---\" % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LEU2CTtlWRL_"
   },
   "outputs": [],
   "source": [
    "# queued_data_rows = [dr['id'] for dr in list(project.export_queued_data_rows())]\n",
    "# data_rows = [dr.uid for dr in list(dataset.export_data_rows())]\n",
    "# data_rows_not_queued = list(set(data_rows)- set(queued_data_rows))\n",
    "\n",
    "# # Randomly select 200 Data Rows\n",
    "# sampled_data_rows = random.sample(data_rows_not_queued, 200)\n",
    "\n",
    "# batch = project.create_batch(\n",
    "#   \"Second batch\", # name of the batch\n",
    "#   sampled_data_rows, # list of Data Rows\n",
    "#   5 # priority between 1-5\n",
    "# )\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Getting Started: Import a labeled dataset.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "a6317aee186ade54f14eb08376491742c71b94e4537d5d56ccac96eb1ad7b948"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
